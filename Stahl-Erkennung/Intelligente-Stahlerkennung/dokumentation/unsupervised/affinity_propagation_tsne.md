# T-distributed Stochastic Neighbor Embedding (t-SNE)

implemented by Gabriel Marcelin

The idea of t-SNE is to generate a representation of high-dimensional data in a low-dimensional space while preserving the structure of the data. This is achieved by defining a probability distribution in the high-dimensional space that indicates the likelihood of two randomly selected points in the space being direct neighbors.

This process allows for preserving complex structures in the data while generating a compact representation that can be used for visualization purposes or dimensionality reduction.

The algorithm is implemented in the file `MLModels/unsupervised/cluster_Algorithm.py`. It utilizes the implementation from `sklearn.manifold`. The function is called using the command `tsne_algorithm(data, perplexity, steps, mode)`.

The function initially employs logarithmic scaling, which could be optional. Then, depending on the previously made selection, the algorithm will produce a result in either a two-dimensional or three-dimensional space.

## Parameters:

- `DataFrame data`: The dataset to be used must be a Pandas DataFrame. The columns should follow the pattern of the previously mentioned scalars if one wishes to use them.
- `Float perplexity`: This parameter indicates how many data points each individual compares to. The higher the value, the clearer the separations of the clusters tend to be. This parameter has a significant impact on the result.
- `Integer steps`: The number of iterations the algorithm goes through.
- `String mode`: A choice between '2D' and '3D' is possible since these results can be plotted.

## Output:

A list containing the results of the algorithm, with two or three values per data point depending on the mode selection.

## Tests and Preprocessing:

There are unit tests to verify the correct selection of parameters and also for the input format. Additionally, there is a Jupyter Notebook where the preceding analysis and determination of parameters can be viewed.

# Affinity Propagation

implemented by Gabriel Marcelin

The idea of Affinity Propagation is based on the principle of message passing. Clusters are generated by exchanging information between pairs of data until they converge. The algorithm autonomously determines the estimated number of clusters and relies on so-called exemplars identified during the message passing.

The proper selection of a distance function strongly influences the result. The parameters used in this implementation have yielded the clearest clusters after numerous runs on the test data.

The algorithm is implemented in the file `MLModels/unsupervised/cluster_Algorithm.py`. It utilizes the implementation from `sklearn.cluster`. The function is called using the command `affinity_propagation(data)`.

## Parameters:

- `DataFrame data`: The dataset to be used must be a Pandas DataFrame. The columns should follow the pattern of the previously mentioned scalars if one wishes to use them.

The remaining parameters are fixed. The `preference` parameter plays the most significant role. After some analysis, it was found that the best results were achieved with `0.6*np.min(-pairwise_distances(df_scaled, metric='l1'))`. However, there is a risk of overfitting. With new data, the constant might need adjustment.

## Output:

The passed DataFrame with an additional column containing the cluster membership of the data as integer values from 0 to k-1.

## Tests and Preprocessing:

There are unit tests to verify the correct selection of parameters and also for the input format. Additionally, there is a Jupyter Notebook where the preceding analysis and determination of parameters can be viewed.

